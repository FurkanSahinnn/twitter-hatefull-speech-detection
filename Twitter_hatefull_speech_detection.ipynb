{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee964c8-79c5-4207-814e-aa019c4138f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "639af4f4-1138-4de3-95a2-c7738e768e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "class DataPreprocessing():\n",
    "    def __init__(self, train_data_path, test_data_path, text_col, label_col):\n",
    "        self.train_data_path = train_data_path\n",
    "        self.test_data_path = test_data_path\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "        self.train_label = None\n",
    "        self.test_label = None\n",
    "        \n",
    "    def read_data(self):\n",
    "        self.train_data = pd.read_csv(self.train_data_path)\n",
    "        self.train_label = self.train_data['label'].values\n",
    "        \n",
    "        self.test_data = pd.read_csv(self.test_data_path)\n",
    "        self.test_label = self.test_data['label'].values\n",
    "        \n",
    "    def drop_id_column(self):\n",
    "        if 'id' in self.train_data.columns:\n",
    "            self.train_data.drop(columns=['id'], inplace=True)\n",
    "        if 'id' in self.test_data.columns:\n",
    "            self.test_data.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    def replace_turkish_chars(self, text):\n",
    "        turkish_chars = {\n",
    "            'ç': 'c',\n",
    "            'Ç': 'C',\n",
    "            'ğ': 'g',\n",
    "            'Ğ': 'G',\n",
    "            'ı': 'i',\n",
    "            'İ': 'I',\n",
    "            'ö': 'o',\n",
    "            'Ö': 'O',\n",
    "            'ş': 's',\n",
    "            'Ş': 'S',\n",
    "            'ü': 'u',\n",
    "            'Ü': 'U'\n",
    "        }\n",
    "        for turkish_char, english_char in turkish_chars.items():\n",
    "            text = text.replace(turkish_char, english_char)\n",
    "        return text\n",
    "        \n",
    "    def clean_tweet(self, text):\n",
    "        # Remove all URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        # Remove all mentions (@User) and Hashtags (#hashtag) with text\n",
    "        text = re.sub(r'@\\w+', '', text)  # Remove words that start with the '@' symbol.\n",
    "        text = re.sub(r'#\\w+', '', text)  # Remove words that start with the '#' symbol.\n",
    "        # Unnecessary punctuation marks.\n",
    "        text = re.sub(r'[^a-zA-Z0-9çÇğĞıİöÖşŞüÜ\\s.,;:!\\'^\\?]', '', text)\n",
    "        # Remove extra space.\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        # Convert lowercase.\n",
    "        text = text.lower()\n",
    "        # Replace Turkish Characters.\n",
    "        text = self.replace_turkish_chars(text)\n",
    "        return text\n",
    "        \n",
    "    def apply_clean_tweet(self):\n",
    "        \"\"\"\n",
    "        # Remove NaN values\n",
    "        self.train_data.dropna(subset=[self.text_col], inplace=True)\n",
    "        self.test_data.dropna(subset=[self.text_col], inplace=True)\n",
    "        \"\"\"\n",
    "        self.train_data['cleaned_text'] = self.train_data[self.text_col].apply(self.clean_tweet)\n",
    "        self.test_data['cleaned_text'] = self.test_data[self.text_col].apply(self.clean_tweet)\n",
    "\n",
    "    def tokenization(self):\n",
    "        self.train_data['token_text'] = self.train_data['cleaned_text'].apply(lambda x: word_tokenize(x))\n",
    "        self.test_data['token_text'] = self.test_data['cleaned_text'].apply(lambda x: word_tokenize(x))\n",
    "    \n",
    "    def remove_stopwords(self):\n",
    "        stopWords = set(stopwords.words('turkish'))\n",
    "        self.train_data['tokens_no_stopwords'] = self.train_data['token_text'].apply(lambda x: [word for word in x if word.lower() not in stopWords])\n",
    "        self.test_data['tokens_no_stopwords'] = self.test_data['token_text'].apply(lambda x: [word for word in x if word.lower() not in stopWords])\n",
    "\n",
    "    def join_column(self):\n",
    "        self.train_data['joined_token_no_stopwords'] = self.train_data['tokens_no_stopwords'].apply(lambda x: ' '.join(x))\n",
    "        self.test_data['joined_token_no_stopwords'] = self.test_data['tokens_no_stopwords'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    def tfidf_feature_extraction(self):\n",
    "        self.join_column()\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        X_tfidf = tfidf_vectorizer.fit_transform(self.train_data['joined_token_no_stopwords'])\n",
    "        # Convert TF-IDF matrix to numpy array.\n",
    "        return X_tfidf.toarray()\n",
    "        \n",
    "    def exec(self):    \n",
    "        self.read_data()\n",
    "        self.drop_id_column()\n",
    "        self.apply_clean_tweet()\n",
    "        self.tokenization()\n",
    "        self.remove_stopwords()\n",
    "        return self.tfidf_feature_extraction(), self.test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57c1d98d-1cd9-4129-85f0-605b663c0f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim verisi boyutu: torch.Size([33918, 5000])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataPreprocessing = DataPreprocessing('dataset/train.csv', 'dataset/test.csv', 'text', 'label')\n",
    "\n",
    "train_data, test_data = dataPreprocessing.exec()\n",
    "\n",
    "X = train_data\n",
    "y = dataPreprocessing.train_label\n",
    "\n",
    "# Veriyi eğitim ve test setlerine ayır\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# PyTorch Tensor'larına dönüştür\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"Eğitim verisi boyutu:\", X_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f1c89de-2825-41b8-95c4-0b107c0f0b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.001):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, X_train, y_train, epoch_size=100):\n",
    "        # Set the mode to train\n",
    "        self.train()\n",
    "        \n",
    "        # Training Loop\n",
    "        for epoch in range(epoch_size):\n",
    "            # Forward pass\n",
    "            outputs = self(X_train)\n",
    "            loss = self.criterion(outputs, y_train)\n",
    "            \n",
    "            # Backpropagation) and Optimizer\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Print the loss every 10 epochs.\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{epoch_size}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        # Set the mode to evaluation\n",
    "        # Equivalent to model.train(False)\n",
    "        self.eval()\n",
    "        \n",
    "        # Test verisiyle tahmin yapma\n",
    "        # Make prediction with test data\n",
    "        with torch.no_grad():\n",
    "            test_outputs = self(X_test)\n",
    "            _, predicted = torch.max(test_outputs, 1)\n",
    "            accuracy = (predicted == y_test).float().mean()\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        return accuracy\n",
    "        \n",
    "    def download(self, PATH):\n",
    "        # torch.save(model.state_dict(), PATH)\n",
    "        torch.save(self.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ffdd20-6c8f-49f5-b47a-2af39cebbdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6715\n",
      "Epoch [20/100], Loss: 0.6341\n",
      "Epoch [30/100], Loss: 0.5880\n",
      "Epoch [40/100], Loss: 0.5364\n",
      "Epoch [50/100], Loss: 0.4846\n",
      "Epoch [60/100], Loss: 0.4380\n",
      "Epoch [70/100], Loss: 0.3994\n",
      "Epoch [80/100], Loss: 0.3689\n",
      "Epoch [90/100], Loss: 0.3449\n",
      "Epoch [100/100], Loss: 0.3257\n",
      "Accuracy: 0.8117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8117)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Parameters\n",
    "input_dim = X_train_tensor.shape[1]  # Input Layer Dimension\n",
    "hidden_dim = 64  # Hidden Layer Dimension\n",
    "output_dim = len(set(y))  # Output Layer Length (or Label Size)\n",
    "\n",
    "# Define Model\n",
    "model = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train_tensor, y_train_tensor, epoch_size=100)\n",
    "\n",
    "# Evaluate Model\n",
    "model.evaluate(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ba9dcf9e-34b4-4c07-9bea-89f9797b44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.download(\"models/twitter_hatefull_speech_detection_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
